{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7107b9d-b3b3-4857-862a-868eab7a671b",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "This week, we will learn to implement artificial neural networks, another class of models that can be used in molecular property prediction. \n",
    "\n",
    "![](mldd_diagram_lab3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bc77c",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "Three components are necessary to train a neural network:\n",
    "1. neural network architecture\n",
    "2. loss function\n",
    "3. optimization algorithm\n",
    "\n",
    "All these components can be easily implemented using one of the deep learning libraries for Python, e.g. PyTorch, TensorFlow, or JAX. You can find also some higher-level libraries built on top of these low-level libraries, e.g. Kera or Sonnet for TensorFlow or Haiku for JAX. We will be using PyTorch and PyTorch-Geometric to build graph neural networks.\n",
    "\n",
    "\n",
    "## Neural network architecture: model capacity and inductive bias\n",
    "\n",
    "Neural networks usually consist of layers, i.e. a network is a sequence of neural layers that transform the input representation. The most basic layer in neural networks is probably a **linear** or **fully-connected layer**. All input features are connected to all layer output features (neurons), and the strength of these connections is learned when the network is trained. Mathematically, we can write:\n",
    "\n",
    "$$\n",
    "[x_1, \\dots, x_n] \\cdot \\begin{bmatrix} \n",
    "w_{11} & \\cdots & w_{1m} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n1} & \\cdots & w_{nm}\n",
    "\\end{bmatrix} = \\mathbf{x}^T W = \\mathbf{y}^T =[y_1, \\dots, y_m],\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}\\in\\mathbb{R}^n$ is the layer input, and $\\mathbf{y}\\in\\mathbb{R}^m$ is the layer output. \n",
    "$W\\in\\mathbb{R}^{n\\times m}$ are layer parameters called **weights**. Usually a **bias** term is added to the transformed output, i.e. $\\mathbf{x}^T W + \\mathbf{b}^T$. Let's note that single output neurons follow the equation similar to linear regression:\n",
    "\n",
    "$$\n",
    "y_k = x_1 w_{1k} + x_2 w_{2k} + \\cdots + x_n w_{nk} + b_k\n",
    "$$\n",
    "\n",
    "Another important class of layers are **non-linear layers** (or **activation layers**). They usually don't contain trainable parameters and are used only to introduce non-linearity to the model. Without them, only linear depenencies could be learned, so the network wouldn't be able to learn anything more than a simple linear regression. Currently, the most widely used activation is **ReLU** (Rectified Linear Unit) because of its effectiveness in terms of the computational cost and training outcomes. The ReLU layer is defined as:\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\max(x,0) = \\begin{cases} x & \\text{dla } x \\geq 0,\\\\ 0 & \\text{dla } x < 0. \\end{cases}\n",
    "$$\n",
    "\n",
    "Another important type of non-linearity is **sigmoid**, which is less effective computation-wise. Sigmoid was used also in the logistic regression for binary classification. It returns numbers in the range (0, 1), which can be treated as positive class probabilities.\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "## Loss function: problem definition and optimization target\n",
    "\n",
    "The neural architecture defines the transformations that the network is capable of performing, but it doesn't define the training task itself. To determine what our network should learn, we will need a **loss function**, which is an optimization target. It's a function that will me minimized (rarely maximized) by modifying network parameters (weights).\n",
    "\n",
    "For example, for the regression problem, a standard loss function is the mean squared error (MSE) that measures the average error of all predictions, squared:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathcal{X}|\\Theta)=\\mathrm{MSE}(\\mathcal{X}|\\Theta) = \\frac{1}{|\\mathcal{X}|} \\sum_{i=1}^{|\\mathcal{X}|} e_i^2 = \\frac{1}{|\\mathcal{X}|} \\sum_{i=1}^{|\\mathcal{X}|} (y_i - \\hat{y}_i)^2 = \\frac{1}{|\\mathcal{X}|} \\sum_{i=1}^{|\\mathcal{X}|} (y_i - f(\\mathbf{x}_i|\\Theta))^2\n",
    "$$ \n",
    "\n",
    "We'll be minimizing this function with respect to the parameters $\\Theta$, which are network weights.\n",
    "\n",
    "$$\n",
    "\\Theta^* = {\\arg \\min}_\\Theta \\,\\, \\mathcal{L}(\\mathcal{X}|\\Theta)\n",
    "$$\n",
    "\n",
    "## Optimization algorithm: method to reach the minimum of the loss function\n",
    "\n",
    "Having defined the architecture and loss function, we need to choose an algorithm that will find optimal model parameters. In the classical machine learning, we oftentimes analytically derived the formulas to find optimal parameters. The diversity and complexity of neural networks make it infeasible to find such solutions for these models.\n",
    "\n",
    "However, we can use **gradient optimization** methods, which compute the loss function gradients (derivatives) w.r.t. the model parameters and move these parameters towards the direction opposite to the gradient (if our goal is to minimize the loss function). It is possible because of the differentiability of all operations in a neural network and the **chain rule**.\n",
    "\n",
    "The most standard algorithm is **SGD** (Stochastic Gradient Descent):\n",
    "\n",
    "$$\n",
    "\\Theta \\leftarrow \\Theta - \\eta \\nabla \\mathcal{L}(\\mathcal{X}|\\Theta),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is learning rate, which can be selected for a specific task (it's a hyperparameter). The SGD method is the most basic, and many follow-up methods have been implemented in deep learning libraries. For example, **Momentum** is a technique that accumulates past gradients to accelerate the convergence and to avoid local minima.\n",
    "\n",
    "$$\n",
    "v \\leftarrow \\gamma v + \\eta \\nabla \\mathcal{L}(\\mathcal{X}|\\Theta), \\\\\n",
    "\\Theta \\leftarrow \\Theta - v.\n",
    "$$\n",
    "\n",
    "The momentum constant $\\gamma$ is usually set to 0.9 (it should be less than 1 to gradually decrease the momentum).\n",
    "\n",
    "**Exercise 1:** Fill in the neural network training code by implementing the three training components mentioned above. This code uses Morgan fingerprints (ECFP) to predict the compound solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tdc.single_pred.adme import ADME\n",
    "from tdc import Evaluator\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self, y_column, smiles_col='Drug', **kwargs):\n",
    "        self.y_column = y_column\n",
    "        self.smiles_col = smiles_col\n",
    "        self.__dict__.update(kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ECFPFeaturizer(Featurizer):\n",
    "    def __init__(self, y_column, radius=2, length=1024, **kwargs):\n",
    "        self.radius = radius\n",
    "        self.length = length\n",
    "        super().__init__(y_column, **kwargs)\n",
    "    \n",
    "    def __call__(self, df):\n",
    "        fingerprints = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, self.radius, nBits=self.length)\n",
    "            fingerprints.append(fp)\n",
    "            labels.append(y)\n",
    "        fingerprints = np.array(fingerprints)\n",
    "        labels = np.array(labels)\n",
    "        return fingerprints, labels\n",
    "\n",
    "data = ADME('Solubility_AqSolDB')\n",
    "split = data.get_split()\n",
    "rmse = Evaluator(name = 'RMSE')\n",
    "\n",
    "featurizer = ECFPFeaturizer(y_column='Y')\n",
    "X_train, y_train = featurizer(split['train'])\n",
    "X_valid, y_valid = featurizer(split['valid'])\n",
    "X_test, y_test = featurizer(split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1c4c4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-10T15:38:30.775926Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_valid, y_valid):\n",
    "    # hyperparameters definition\n",
    "    hidden_size = 512\n",
    "    epochs = 50\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # model preparation\n",
    "    model = nn.Sequential(\n",
    "        torch.nn.Linear(1024, hidden_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_size, 1)\n",
    "    ) # TODO: define a simple neural network, you can use torch.nn.Sequential\n",
    "    \n",
    "    # data preparation\n",
    "    dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train.reshape(-1, 1)))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.FloatTensor(y_valid.reshape(-1, 1)))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # prepare metrics plots\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(7, 3), layout=\"constrained\")\n",
    "    dh = display.display(fig, display_id=True)\n",
    "    df_metrics = pd.DataFrame()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(),momentum=0.9)  # TODO: define an optimizer\n",
    "    loss_fn =  torch.nn.MSELoss() # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        model.train()\n",
    "        for X, y in tqdm(loader, leave=False):\n",
    "            model.zero_grad()\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # validation loop\n",
    "        model.eval()\n",
    "        preds_batches = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in tqdm(valid_loader, leave=False):\n",
    "                preds = model(X)\n",
    "                preds_batches.append(preds.cpu().detach().numpy())\n",
    "        preds = np.concatenate(preds_batches)\n",
    "        rmse_score = rmse(y_valid, preds)\n",
    "        \n",
    "        df_metrics = pd.concat([df_metrics, pd.DataFrame({'epoch': epoch, 'loss': loss.item(), 'rmse': rmse_score}, index=[0])], ignore_index=True)\n",
    "        ax[0].clear()\n",
    "        ax[0].plot(df_metrics.epoch, df_metrics.loss)\n",
    "        ax[0].set_title('training loss')\n",
    "        ax[0].set_xlabel('epoch')\n",
    "        ax[0].set_ylabel('MSE')\n",
    "        ax[1].clear()\n",
    "        ax[1].plot(df_metrics.epoch, df_metrics.rmse)\n",
    "        ax[1].set_title('validation RMSE')\n",
    "        ax[1].set_xlabel('epoch')\n",
    "        ax[1].set_ylabel('RMSE')\n",
    "        dh.update(fig)\n",
    "    plt.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, X_test, y_test):\n",
    "    # hyperparameters definition\n",
    "    # (but this doesn't change the training results, it's only to optimize the eval speed)\n",
    "    batch_size = 64\n",
    "\n",
    "    # data preparation\n",
    "    dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test.reshape(-1, 1)))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # evaluation loop\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(loader):\n",
    "            preds = model(X)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds\n",
    "\n",
    "# training\n",
    "model = train(X_train, y_train, X_valid, y_valid)\n",
    "\n",
    "# evaluation\n",
    "predictions = predict(model, X_test, y_test)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions)\n",
    "print(f'RMSE = {rmse_score:.3f}')\n",
    "assert rmse_score < 1.6, \"It should be possible to obtain RMSE lower than 1.6\"\n",
    "print('Looks OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5a217d",
   "metadata": {},
   "source": [
    "# Molecular graphs\n",
    "\n",
    "**Recap:** In mathematics, a graph is an object that consists of a set of vertices (nodes) connected with edges, i.e. $\\mathcal{G} = (V, E)$, where $V = \\{ v_i: i \\in \\{1, 2, \\dots, N \\} \\}$ and $E \\subseteq \\{ (v_i, v_j):\\, v_i,v_j \\in V \\}$.\n",
    "\n",
    "Molecular graphs are a special class of graphs, where besides nodes (denoting atoms) and edges (denoting chemical bonds), we have an additional information about atom types and sometimes also bond types. We can assume that we have an additional set of node/atom features encoded as a matrix $X$, where $X_{ij}$ is the $j$-th feature of the $i$-th atom. As atomic features, we can have one-hot encoded atom symbols (a vector containing zeros on all positions besides the position that corresponds to the atom symbol), the number of implicit hydrogens bonded with this atom, or the number of heavy neighbors (atoms other than hydrogens bonded to the given atom).\n",
    "\n",
    "Egdes/bonds can be encoded in two different ways. One method is to use an adjacency matrix $A$, where $A_{ij}=1$ if nodes/atoms $v_i$ nad $v_j$ are connected ($A_{ij}=0$ otherwise). In the case of sparse matrices, a more useful encoding is a list of pairs of connected atoms (a list of index pairs). This latter enocding is used by the PyTorch-Geometric library.\n",
    "\n",
    "In practice, a molecular graph can be described by two matrices: $X \\in \\mathbb{R}^{N \\times F}$ and $E \\in \\{0, 1,\\dots,N-1\\}^{2 \\times N}$, where $N$ is the number of atoms, and $F$ is the number of atomic features.\n",
    "\n",
    "![molecular graph](../../lectures/assets/mol_graph.png)\n",
    "\n",
    "**Exercise 2:** Create a molecular graph dataset using PyTorch-Geometric and the same data as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7f43d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-10T15:38:43.128378Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise ValueError(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "\n",
    "class GraphFeaturizer(Featurizer):\n",
    "    def __call__(self, df):\n",
    "        graphs = []\n",
    "        labels = []\n",
    "        for i, row in df.iterrows():\n",
    "            y = row[self.y_column]\n",
    "            smiles = row[self.smiles_col]\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            \n",
    "            edges = []\n",
    "            for bond in mol.GetBonds():\n",
    "                edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "                edges.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()])\n",
    "            edges = np.array(edges)\n",
    "            \n",
    "            nodes = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                results = one_of_k_encoding_unk(\n",
    "                    atom.GetSymbol(),\n",
    "                    ['C','N','O','Cl','Br','I','S','unk']\n",
    "                ) + one_of_k_encoding_unk(\n",
    "                    atom.GetDegree(),\n",
    "                    [0,1,2,3,4,-1]\n",
    "                ) + one_of_k_encoding_unk(\n",
    "                    atom.GetNumImplicitHs(),\n",
    "                    [0,1,2,3,4,-1]\n",
    "                )# TODO: Add atom features as a list, you can use one_of_k_encodings defined above\n",
    "                nodes.append(results)\n",
    "            nodes = np.array(nodes)\n",
    "            \n",
    "            graphs.append((nodes, edges.T))\n",
    "            labels.append(y)\n",
    "        labels = np.array(labels)\n",
    "        return [Data(\n",
    "            x=torch.FloatTensor(x), \n",
    "            edge_index=torch.LongTensor(edge_index), \n",
    "            y=torch.FloatTensor([y])\n",
    "        ) for ((x, edge_index), y) in zip(graphs, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d4db6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-10T15:38:43.295052Z"
    }
   },
   "outputs": [],
   "source": [
    "featurizer = GraphFeaturizer('Y')\n",
    "graph = featurizer(split['test'].iloc[:1])[0]\n",
    "\n",
    "assert graph.x.ndim == 2, 'The atom features should be a matrix with dimensions (number of atoms) x (number of features)'\n",
    "assert graph.edge_index.ndim == 2, 'The edges should be represented as a matrix of atom pairs'\n",
    "assert graph.edge_index.shape[0] == 2, 'The first dimension should be 2 (we need atom pairs)'\n",
    "assert isinstance(graph.y, torch.FloatTensor) and graph.y.shape == (1,), 'The graph label should be assigned to the variable `y`'\n",
    "print('Looks OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566ac82-f64c-41de-a36d-25921f02c976",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between\">\n",
    "    <div style=\"width: 20%; display: inline-block; margin: 20px\">\n",
    "        <img src=\"../../assets/profile1.png\" width=\"100%\">\n",
    "    </div>\n",
    "    <div style=\"width: 60%; display: inline-block; margin: 20px\">\n",
    "        <p><strong>Nitro:</strong> Atoms possess several important properties that define their behavior in chemical systems. The <strong>atomic number</strong> of an atom signifies the number of protons in its nucleus, which determines its elemental identity and placement on the periodic table. <strong>Formal charge</strong> represents the charge assigned to an atom in a molecule or ion, calculated by considering the number of valence electrons and its bonding situation. <strong>Partial charge</strong> refers to the uneven distribution of electron density within a molecule, resulting in regions of slight positive or negative charge. <strong>Hybridization</strong> is a concept that describes the mixing of atomic orbitals to form hybrid orbitals, crucial for understanding molecular geometry and bonding patterns. These properties collectively influence an atom's reactivity, bonding behavior, and its role within chemical compounds, providing insight into the complex nature of molecular interactions.</p>\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"width: 80%; display: block; margin: 0 20px 0\">\n",
    "    <p>In typical druglike compounds, a diverse array of atom types can be found, each playing a crucial role in the compound's pharmacological activity. <strong>Carbon</strong> atoms are ubiquitous, forming the backbone of organic molecules and providing structural stability. <strong>Hydrogen</strong> atoms frequently accompany carbon, contributing to molecular stability and participating in various bonding interactions. <strong>Oxygen</strong> atoms are common, often found in functional groups such as hydroxyl (-OH) and carbonyl (C=O), influencing the compound's polarity and reactivity. <strong>Nitrogen</strong> atoms, present in amines and amides, contribute to the compound's basicity and ability to form hydrogen bonds. Additionally, <strong>sulfur</strong> atoms may appear in thiol (-SH) or sulfide (-S-) groups, introducing potential sites for redox reactions or metal binding. Halogens, such as <strong>fluorine</strong>, <strong>chlorine</strong>, <strong>bromine</strong>, and <strong>iodine</strong>, are also commonly encountered in druglike compounds, where they can serve as substituents or functional groups, imparting specific chemical properties and influencing the compound's biological activity and metabolic stability. These diverse atom types collectively contribute to the complex pharmacological properties of druglike compounds, influencing their efficacy, safety, and pharmacokinetic profiles.</p>\n",
    "    </div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051f97",
   "metadata": {},
   "source": [
    "# Graph neural networks\n",
    "\n",
    "Graph neural networks (GNNs) can process graph representation given at the input to the model. A graph layer uses the input atom features and graph topology to calculate new atom representations, but the graph structure is not changed (the edges stay the same). We can use the calculated atom representations for **node classification**, or we can add a graph readout operation at the end to aggregate information from all nodes into one vector that describes the whole graph (e.g. we can use the average atom representation). The graph representation can be then processed by fully-connected layers for the **graph classification** task.\n",
    "\n",
    "\n",
    "## Message Passing Neural Networks (MPNN)\n",
    "\n",
    "MPNN is a general description of a graph neural network, and many graph neural network architectures can be matched with this description (including the ones listed below). In MPNN, **messages** $M$ from the neighboring atoms are retrieved to calculate a new atom representation via the **update** $U$ operation. This procedure is repeated a few times before we use **readout** $R$ to compute the graph representation.\n",
    "\n",
    "$$\n",
    "\\mathbf{m}_i^{t+1} = \\sum_{j\\in\\mathcal{N}(i)} M_t(\\mathbf{h}_i^t, \\mathbf{h}_j^t, \\mathbf{e}_{ij})\\\\\n",
    "\\mathbf{h}_i^{t+1} = U_t(\\mathbf{h}_i^t, \\mathbf{m}_i^{t+1}) \\\\\n",
    "\\hat{y} = R(\\{\\mathbf{h}_i^T \\, | \\, i \\in \\mathcal{G} \\})\n",
    "$$\n",
    "\n",
    "- **Graph Convolutional Networks (GCN)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = W^T \\sum_{j\\in\\mathcal{N}(i)\\cup \\{i\\}} \\frac{e_{ij}}{\\sqrt{\\hat{d}_i \\hat{d}_j}} \\mathbf{h}_j^t\n",
    "$$\n",
    "\n",
    "- **Graph Isomorphism Networks (GIN)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = W^T \\left( (1+\\varepsilon)\\mathbf{h}_i^t + \\sum_{j\\in\\mathcal{N}(i)} \\mathbf{h}_j^t \\right)\n",
    "$$\n",
    "\n",
    "- **GraphSAGE**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = W_1 \\mathbf{h}_i^t + W_2 \\frac{1}{|\\mathcal{N}(i)|} \\sum_{j\\in\\mathcal{N}(i)} \\mathbf{h}_j^t\n",
    "$$\n",
    "\n",
    "- **Graph Attention Networks (GAT)**\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_i^{t+1} = \\sum_{j\\in\\mathcal{N}(i)\\cup \\{i\\}} \\alpha_{ij} W \\mathbf{h}_j^t,\\\\\n",
    "\\alpha_{ij} = \\frac{\\exp\\left( LeakyReLU(\\mathbf{a}[W\\mathbf{h}_i^t \\| W\\mathbf{h}_j^t])\\right)}{\\sum_{k\\in\\mathcal{N}(i) \\cup \\{i\\}}\\exp\\left( LeakyReLU(\\mathbf{a}[W\\mathbf{h}_i^t \\| W\\mathbf{h}_k^t])\\right)}\n",
    "$$\n",
    "\n",
    "**Exercise 3:** Use the molecular graphs prepared in the previous exercise to predict compound solubility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6b6db",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-10T15:38:43.302467Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "batch_size = 64\n",
    "train_loader = GraphDataLoader(featurizer(split['train']), batch_size=batch_size, shuffle=True)\n",
    "valid_loader = GraphDataLoader(featurizer(split['valid']), batch_size=batch_size)\n",
    "test_loader = GraphDataLoader(featurizer(split['test']), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb8d9e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-10T15:39:11.136Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraphNeuralNetwork(torch.nn.Module):  # TODO: assign hyperparameters to attributes and define the forward pass\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.gcn1=GCNConv(8 + 6 + 6, hidden_size)  # TODO: copy from Exercise 3\n",
    "        self.gcn2=GCNConv(hidden_size,hidden_size)\n",
    "        self.gcn3=GCNConv(hidden_size,hidden_size)\n",
    "        self.gcn4=GCNConv(hidden_size,hidden_size)\n",
    "        self.lin=nn.Linear(hidden_size,1)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        y = self.gcn1(x,edge_index)\n",
    "        y = F.relu(y)\n",
    "        y = self.gcn2(y,edge_index)\n",
    "        y = F.relu(y)\n",
    "        y = self.gcn3(y,edge_index)\n",
    "        y = F.relu(y)\n",
    "        y = self.gcn4(y,edge_index)\n",
    "        y = global_mean_pool(y,batch)\n",
    "        y = self.lin(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7be6c09a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:39:16.272598Z",
     "start_time": "2024-04-10T16:38:56.349345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ec242eb04c64dc193d7e856f99be3f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da0465e5513545309c90c0a7d87c93b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44d52f98b3594557af66f3c9d068cd33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5667b17675ba4840b4fb0cb63668c00f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b07a582c42c9440091617de1e5a3ae15"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4e5a5ff29d14368a6e794e976ba1815"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db44830292e249d987abdf6e56ae92fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea75c616cb2544fdb32725f16d3db876"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ae70254c2c24c659465ac758016a6a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f368d0dff7b148979bad631e5d175954"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2b1f6d804154fa687c7f071c2a51cc2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ef6453801324dc19eb2059746301edc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "040238f617624d76a2852a6d5fac2b61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ac0fa2789844aeaa4b9915c32e1db2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "429675fe6b1d473eb823790d7b003e88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2e3ec9701f74edf9cd9b762ad117a21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e29bde0aceb407d9c479c405640e135"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f15f87c9ad74ad19afd0bb071e6ef27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96690e1538694d3797c22f2da66d261b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db0ad433d43f49ba99ac9067de19d3ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8db720e3933e44949b65608579e8f4e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "268aef3be13a44ebba3b145abbe15506"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 2.35\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "It should be possible to obtain RMSE lower than 1.4",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 65\u001B[0m\n\u001B[1;32m     62\u001B[0m rmse_score \u001B[38;5;241m=\u001B[39m rmse(y_test, predictions\u001B[38;5;241m.\u001B[39mflatten())\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRMSE = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrmse_score\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m rmse_score \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1.4\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt should be possible to obtain RMSE lower than 1.4\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLooks OK!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     67\u001B[0m df_logs\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining_logs.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: It should be possible to obtain RMSE lower than 1.4"
     ]
    }
   ],
   "source": [
    "df_logs = pd.DataFrame()\n",
    "\n",
    "def train(train_loader, valid_loader, df_logs):\n",
    "    # hyperparameters definition\n",
    "    hidden_size = 512\n",
    "    epochs = 10\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # model preparation\n",
    "    model = GraphNeuralNetwork(hidden_size)  # TODO: you can add more hyperparameters if needed\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        model.train()\n",
    "        for data in tqdm(train_loader, leave=False):\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation loop\n",
    "        preds = predict(model, valid_loader)\n",
    "        df_logs = pd.concat([df_logs, pd.DataFrame(\n",
    "            {'epoch': epoch,\n",
    "             'preds': preds.flatten(),\n",
    "             'mode': 'valid'\n",
    "        }, index=range(len(preds.flatten())))], ignore_index=True)\n",
    "    \n",
    "    return model, df_logs\n",
    "\n",
    "\n",
    "def predict(model, test_loader):\n",
    "    # evaluation loop\n",
    "    model.eval()\n",
    "    preds_batches = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "            \n",
    "            preds = model(x, edge_index, batch)\n",
    "            preds_batches.append(preds.cpu().detach().numpy())\n",
    "    preds = np.concatenate(preds_batches)\n",
    "    return preds\n",
    "\n",
    "\n",
    "# training\n",
    "model, df_logs = train(train_loader, valid_loader, df_logs)\n",
    "\n",
    "# evaluation\n",
    "predictions = predict(model, test_loader)\n",
    "df_logs = pd.concat([df_logs, pd.DataFrame({\n",
    "    'epoch': -1,\n",
    "    'preds': predictions.flatten(),\n",
    "    'mode': 'test'\n",
    "}, index=range(len(predictions.flatten())))], ignore_index=True)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "\n",
    "print(f'RMSE = {rmse_score:.2f}')\n",
    "assert rmse_score < 1.4, \"It should be possible to obtain RMSE lower than 1.4\"\n",
    "print('Looks OK!')\n",
    "df_logs.to_csv('training_logs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cdc10",
   "metadata": {},
   "source": [
    "# Explainability of GNN predictions: Grad-CAM\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between\">\n",
    "    <div style=\"width: 20%; display: inline-block; margin: 20px\">\n",
    "        <img src=\"../../assets/profile3.png\" width=\"100%\">\n",
    "    </div>\n",
    "    <div style=\"width: 60%; display: inline-block; margin: 20px\">\n",
    "        <p><strong>Orion:</strong> Explainable artificial intelligence (XAI) is vital across various domains, especially in fields where decision-making involves complex systems like molecular and atomic interactions. In molecular science, understanding the reasoning behind AI predictions is crucial for elucidating intricate molecular behaviors and properties. XAI techniques enable researchers to interpret how molecular features and atomic-level interactions contribute to predictions, offering valuable insights into molecular structures, dynamics, and properties. This transparency not only enhances the reliability of AI-driven predictions but also facilitates knowledge discovery and hypothesis generation in areas such as drug discovery, material design, and chemical synthesis.</p>\n",
    "    </div>\n",
    "</div>\n",
    "<div style=\"width: 80%; display: block; margin: 0 20px 0\">\n",
    "    <p>Attribution and gradient techniques are fundamental components of explainable artificial intelligence (XAI), crucial for unraveling the intricate decision-making processes of complex machine learning models. Attribution methods, such as Integrated Gradients and Layer-wise Relevance Propagation (LRP), aim to attribute the importance of input features to the model's output, offering insights into which features drive the predictions. These techniques help in understanding the model's decision rationale, particularly valuable in domains where transparency is paramount, such as healthcare and finance. On the other hand, gradient-based methods, including saliency maps and gradient-weighted class activation mapping (Grad-CAM), visualize the regions of input data that significantly influence model predictions by analyzing the gradient flow during backpropagation. Together, these techniques provide researchers and practitioners with interpretable insights into model behavior, fostering trust, facilitating debugging, and guiding model improvement efforts in various applications.</p>\n",
    "    </div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In many applications of machine learning, it is crucial to understand model predictions. For example, it is important to understand why our model predicted toxicity if we want to improve the chemical structure and get rid of this liability. In graph neural networks, we can create an importance map overlayed on top of the chemical structure to point at the atoms important for the prediction.\n",
    "\n",
    "Grad-CAM is an explainability method developed for convolutional neural networks (images), and can be adapted to graph convolutions. The representation after the last graph layer is called an **activation map** $F$, where $F_{i,j}$ represents the $i$-th feature of the $j$-th atom. To find out which atomic features correlate with the prediction, we can compute gradients of the predicted class probability w.r.t. the activation map features. Next, these gradients $\\alpha$ are multiplied by the values in the activation maps to calculate per-atom importances.\n",
    "\n",
    "$$\n",
    "\\alpha_k^{l,c}=\\frac{1}{N}\\sum_{n=1}^N \\frac{\\partial y^c}{\\partial F_{k,n}^l},\\\\\n",
    "L^c(l,n) = ReLU\\left(\\sum_k \\alpha_k^{l,c} F_{k,n}^l (X, A)\\right)\n",
    "$$\n",
    "\n",
    "**(optional) Exercise 4.** Copy the graph network code from the previous exercise into correct places, train this modified network, and complete the Grad-CAM code to see the prediction explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e52ae890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:40:44.594038Z",
     "start_time": "2024-04-10T16:40:44.590396Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gcn1 = GCNConv(8 + 6 + 6, hidden_size)\n",
    "        self.bn1 = BatchNorm1d(hidden_size)\n",
    "        self.gcn2 = GCNConv(hidden_size, hidden_size)\n",
    "        self.bn2 = BatchNorm1d(hidden_size)\n",
    "        self.gcn3 = GCNConv(hidden_size, hidden_size)\n",
    "        self.bn3 = BatchNorm1d(hidden_size)\n",
    "        self.gcn4 = GCNConv(hidden_size, hidden_size)\n",
    "        self.bn4 = BatchNorm1d(hidden_size)\n",
    "        self.lin = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def activations_hook(self, grad):\n",
    "        self.final_conv_grads = grad\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # TODO: copy from Exercise 3\n",
    "        with torch.enable_grad():\n",
    "            y = self.gcn1(x,edge_index)\n",
    "            y = F.relu(y)\n",
    "            y = self.gcn2(y,edge_index)\n",
    "            y = F.relu(y)\n",
    "            y = self.gcn3(y,edge_index)\n",
    "            y = F.relu(y)\n",
    "            y = self.gcn4(y,edge_index)\n",
    "            y = global_mean_pool(y,batch)\n",
    "            y = self.lin(y)\n",
    "            return y"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, df_logs):\n",
    "    # hyperparameters definition\n",
    "    hidden_size = 512\n",
    "    epochs = 25\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # model preparation\n",
    "    model = GraphNeuralNetwork(hidden_size)  # TODO: you can add more hyperparameters if needed\n",
    "    model.train()\n",
    "    \n",
    "    # training loop\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate) # TODO: define an optimizer\n",
    "    loss_fn = nn.MSELoss()  # TODO: define a loss function\n",
    "    for epoch in trange(1, epochs + 1, leave=False):\n",
    "        model.train()\n",
    "        for data in tqdm(train_loader, leave=False):\n",
    "            x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
    "            model.zero_grad()\n",
    "            preds = model(x, edge_index, batch)\n",
    "            loss = loss_fn(preds, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation loop\n",
    "        preds = predict(model, valid_loader)\n",
    "        df_logs = pd.concat([df_logs, pd.DataFrame(\n",
    "            {'epoch': epoch,\n",
    "             'preds': preds.flatten(),\n",
    "             'mode': 'valid'\n",
    "        }, index=range(len(preds.flatten())))], ignore_index=True)\n",
    "    \n",
    "    return model, df_logs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T16:46:01.397395Z",
     "start_time": "2024-04-10T16:46:01.393644Z"
    }
   },
   "id": "193e14fd6c474e3c",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5804ece0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:49:20.374186Z",
     "start_time": "2024-04-10T16:48:01.671896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/25 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2931b6fe3eb408d89b6ee96afaa71e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b69a486b3ea34b60b2652790b6df9808"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bbeaf6e5ee84d879e0f511e3335dbde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4614011506db457ea002985bd632d614"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6d54bf8aa98405d9a4a721ca38114ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "863de8ea8e25461a84fa5e7eeed02454"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "831bb3b7ccda490ab746db0ac6687e72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cae5938bfbc046e9a508dc792591890c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b954272087c34ec5bc7f9cca385a45f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65a0dd421992495faa5df4840d548921"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e5ac46f2042e441b8c6327f8a6f61199"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0206b5e4743a44ee8ce0387a5c56e994"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51519e76eba94866afe51961e270de89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0069f1ee643d43d3bdb2271d84bd3dfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0d56501fdfc435e84c2ed4e4a40650a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7aef61cab57b4f4caabd188c92f2d5f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fadc2b983f994b53ad944f0b97ed20ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d2248a7f95b47c28176d808efd744f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "854c38192a2848b68a5903f714afd70b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6a6239e8dec48a2b80c55445359a99e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29777736fd7a4c48a74d6ff8d2386617"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23f4d0079d68494e90fc8664d6ff13c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "404d9083111b484d9364ae8ae8a6e631"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3b626e5c77d48c9823deb9fec60483a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7d6c33fd06a4539b23ed72de1f2bd85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58b8546d844c4b55ab859cf2ed657004"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5acd130358541f782bc9050495d4e8f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67911228e3ab4e3bb48f50112e9b6497"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "feae2545458349c09f03c4414b5e12a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0980c5d7db944ceab81923fa884576ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39086698faab42fab01a9a7ca1a3c95c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dfb251c3aeb4e98965908d9d4fdc0fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f665995fa60c42f1bf0f7b3599058b75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93b3c8144873491db0778d74c402dd34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fce285b6229947c794b9543f9c89a69e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bbf6d998b05462ab577abe50b1994f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b91cbe79d8834182bd8769d506a6ee5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f405a5af5e2416a91f9dd50deb54cb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d573fabf79b4a14aa0974c30a20f2b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd6f8b6f7b894cbda72efce174fed047"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b56fc883cba4d018234fe003610cc59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7b560568c564c36ae3b2221c2950c63"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82d383b1d7db48719856e2fb244fa992"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a21966b0f2294ae5b99590fe8e78829c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3dc6a8d09fc4096abcd2bc593ab05cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6da12c11fe6487cbc49d0a33d8f5098"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b15f5837cd241499328b574b998beda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ed81a7323444587bc3bd4a9f7c942be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6145175a6c8c4b9a93943c94a02973f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/110 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b97ca2d9be54a73980e19c08e70b756"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43ca7902b00d45d2950812a532c1b32f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15fe5bfc39f04abf87d8622686920260"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.39\n",
      "Looks OK!\n"
     ]
    }
   ],
   "source": [
    "# TODO: train this network using the script in Exercise 3.\n",
    "# training\n",
    "df_logs = pd.DataFrame()\n",
    "model, df_logs = train(train_loader, valid_loader, df_logs)\n",
    "\n",
    "# evaluation\n",
    "predictions = predict(model, test_loader)\n",
    "df_logs = pd.concat([df_logs, pd.DataFrame({\n",
    "    'epoch': -1,\n",
    "    'preds': predictions.flatten(),\n",
    "    'mode': 'test'\n",
    "}, index=range(len(predictions.flatten())))], ignore_index=True)\n",
    "\n",
    "rmse_score = rmse(y_test, predictions.flatten())\n",
    "\n",
    "print(f'RMSE = {rmse_score:.2f}')\n",
    "assert rmse_score < 1.4, \"It should be possible to obtain RMSE lower than 1.4\"\n",
    "print('Looks OK!')\n",
    "df_logs.to_csv('training_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72c19265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:31:11.831575Z",
     "start_time": "2024-04-10T16:31:11.828223Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def grad_cam(final_conv_acts, final_conv_grads):\n",
    "    node_heat_map = []\n",
    "    alphas = ...  # TODO (formula 1)\n",
    "    for n in range(final_conv_acts.shape[0]): # nth node\n",
    "        node_heat = ...  # TODO (formula 2)\n",
    "        node_heat_map.append(node_heat)\n",
    "    return node_heat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "848a2ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:31:18.135223Z",
     "start_time": "2024-04-10T16:31:12.381028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:31:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:14] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:31:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "test_set = featurizer(split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "657945f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T16:31:18.160218Z",
     "start_time": "2024-04-10T16:31:18.136243Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m x, edge_index, batch \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mx, data\u001B[38;5;241m.\u001B[39medge_index, data\u001B[38;5;241m.\u001B[39mbatch\n\u001B[1;32m     14\u001B[0m model(x, edge_index, torch\u001B[38;5;241m.\u001B[39mzeros(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mint64))\n\u001B[0;32m---> 15\u001B[0m atom_weights \u001B[38;5;241m=\u001B[39m \u001B[43mgrad_cam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinal_conv_acts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinal_conv_grads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m atom_weights \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(atom_weights)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (atom_weights \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.\u001B[39m)\u001B[38;5;241m.\u001B[39many():\n",
      "Cell \u001B[0;32mIn[44], line 7\u001B[0m, in \u001B[0;36mgrad_cam\u001B[0;34m(final_conv_acts, final_conv_grads)\u001B[0m\n\u001B[1;32m      5\u001B[0m node_heat_map \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      6\u001B[0m alphas \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m  \u001B[38;5;66;03m# TODO (formula 1)\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[43mfinal_conv_acts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m[\u001B[38;5;241m0\u001B[39m]): \u001B[38;5;66;03m# nth node\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     node_heat \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m  \u001B[38;5;66;03m# TODO (formula 2)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     node_heat_map\u001B[38;5;241m.\u001B[39mappend(node_heat)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from IPython.display import SVG\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "compound_idx = 110\n",
    "mol = Chem.MolFromSmiles(split['test'].iloc[compound_idx].Drug)\n",
    "\n",
    "data = test_set[compound_idx]\n",
    "x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "model(x, edge_index, torch.zeros(x.shape[0], dtype=torch.int64))\n",
    "atom_weights = grad_cam(model.final_conv_acts, model.final_conv_grads)\n",
    "\n",
    "atom_weights = np.array(atom_weights)\n",
    "if (atom_weights > 0.).any():\n",
    "    atom_weights = atom_weights / atom_weights.max() / 2\n",
    "\n",
    "if len(atom_weights) > 0:\n",
    "    norm = matplotlib.colors.Normalize(vmin=-1, vmax=1)\n",
    "    cmap = cm.get_cmap('bwr')\n",
    "    plt_colors = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    atom_colors = {\n",
    "        i: plt_colors.to_rgba(atom_weights[i]) for i in range(len(atom_weights))\n",
    "    }\n",
    "    highlight_kwargs = {\n",
    "        'highlightAtoms': list(range(len(atom_weights))),\n",
    "        'highlightBonds': [],\n",
    "        'highlightAtomColors': atom_colors\n",
    "    }\n",
    "\n",
    "d = rdMolDraw2D.MolDraw2DSVG(500, 500) # or MolDraw2DCairo to get PNGs\n",
    "rdMolDraw2D.PrepareAndDrawMolecule(d, mol, **highlight_kwargs)\n",
    "d.FinishDrawing()\n",
    "svg = d.GetDrawingText()\n",
    "svg = svg.replace('svg:', '')\n",
    "SVG(svg)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "823a9829a26a304c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
